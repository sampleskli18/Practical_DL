{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This seminar__ teaches you about metric learning for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford question answering dataset (SQuAD)\n",
    "\n",
    "_this seminar is based on original notebook by [Oleg Vasilev](https://github.com/Omrigan/)_\n",
    "\n",
    "Today we are going to work with a popular NLP dataset.\n",
    "\n",
    "Here is the description of the original problem:\n",
    "\n",
    "```\n",
    "Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.\n",
    "```\n",
    "\n",
    "\n",
    "We are not going to solve it :) Instead we will try to answer the question in a different way: given the question, we will find a **sentence** containing the answer, but not within the context, but in a **whole databank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.load(open('train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP part\n",
    "\n",
    "The code here is very similar to `week10/`: preprocess text into tokens, create dictionaries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|\\d+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenize(value):\n",
    "    return tokenizer.tokenize(value.lower())\n",
    "\n",
    "for q in tqdm.tqdm_notebook(data['data']):\n",
    "    for p in q['paragraphs']:\n",
    "        token_counts.update(tokenize(p['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 4\n",
    "\n",
    "tokens = [w for w, c in token_counts.items() if c > min_count] \n",
    "tokens = [\"_PAD_\", \"_UNK_\"] + tokens\n",
    "\n",
    "token_to_id = {t : i for i, t in enumerate(tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[tokens[42]]==42\n",
    "assert len(token_to_id)==len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_ix = token_to_id[\"_PAD_\"]\n",
    "UNK_ix = token_to_id['_UNK_']\n",
    "\n",
    "#good old as_matrix for the third time\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    if isinstance(sequences[0], (str, bytes)):\n",
    "        sequences = [tokenize(s) for s in sequences]\n",
    "        \n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + PAD_ix\n",
    "    for i, seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_ix) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = as_matrix([\"Definitely, thOsE tokens areN'T LowerCASE!!\", \"I'm the monument to all your sins.\"])\n",
    "print(test)\n",
    "assert test.shape==(2,8)\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def build_dataset(train_data):\n",
    "    '''Takes SQuAD data\n",
    "    Returns a list of tuples - a set of pairs (q, a_+)\n",
    "    '''\n",
    "    dataset = []\n",
    "    for row in tqdm.tqdm_notebook(train_data):\n",
    "        for paragraph in row['paragraphs']:\n",
    "            offsets = []\n",
    "            curent_index = 0\n",
    "            for sent in sent_tokenize(paragraph['context']):\n",
    "                curent_index+=len(sent)+2\n",
    "                offsets.append((curent_index, sent))\n",
    "                \n",
    "            for qa in paragraph['qas']:\n",
    "                question, answer = qa['question'], qa['answers'][0]\n",
    "                \n",
    "                #find a sentence that contains an answer\n",
    "                for offset, sent in offsets:\n",
    "                    if answer['answer_start'] < offset:\n",
    "                        dataset.append((question, sent))\n",
    "                        break\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(data['data'], test_size=0.1)\n",
    "\n",
    "train_data = build_dataset(train_data)\n",
    "val_data = build_dataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 18, 6):\n",
    "    print(\"Q: %s\\nA: %s\\n\" % val_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "Any self-respecting DSSM must have one or several vectorizers. In our case,\n",
    "* Context vectorizer\n",
    "* Answer vectorizer\n",
    "\n",
    "It is perfectly legal to share some layers between them, but make sure they are at least a little different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we might as well create a global embedding layer here\n",
    "\n",
    "GLOBAL_EMB = nn.Embedding(len(tokens), 64, padding_idx=PAD_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QuestionVectorizer(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64, use_global_emb=True):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for questions.\n",
    "        Use any combination of layers you want to encode a variable-length input \n",
    "        to a fixed-size output vector\n",
    "        \n",
    "        If use_global_emb is True, use GLOBAL_EMB as your embedding layer\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        if use_global_emb:\n",
    "            self.emb = GLOBAL_EMB\n",
    "        else:\n",
    "            self.emb = <YOUR CODE>\n",
    "            \n",
    "        <YOUR CODE>\n",
    "        \n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "        return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AnswerVectorizer(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64, use_global_emb=True):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for answers.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \n",
    "        If use_global_emb is True, use GLOBAL_EMB as your embedding layer\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        if use_global_emb:\n",
    "            self.emb = GLOBAL_EMB\n",
    "        else:\n",
    "            self.emb = <YOUR CODE>\n",
    "            \n",
    "        <YOUR CODE>\n",
    "        \n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "        return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vectorizer in [QuestionVectorizer(out_size=100), AnswerVectorizer(out_size=100)]:\n",
    "    print(\"Testing %s ...\" % vectorizer.__class__.__name__)\n",
    "    dummy_x = Variable(torch.LongTensor(test))\n",
    "    dummy_v = vectorizer(dummy_x)\n",
    "\n",
    "    assert isinstance(dummy_v, Variable)\n",
    "    assert tuple(dummy_v.shape) == (dummy_x.shape[0], 100)\n",
    "\n",
    "    del vectorizer\n",
    "    print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "question_vectorizer = QuestionVectorizer()\n",
    "answer_vectorizer = AnswerVectorizer()\n",
    "\n",
    "opt = torch.optim.Adam(chain(question_vectorizer.parameters(),\n",
    "                             answer_vectorizer.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a single `encode`, but with different weights. You can use different encode for anchor and negatives/positives.\n",
    "\n",
    "Negative sampling can be either `in-graph` or `out-graph`. We start with out-graph. In the home assignment you are going to use in-graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size=None, replace=False, volatile=False, max_len=None):\n",
    "    \"\"\" Samples training/validation batch with random negatives \"\"\"\n",
    "    if batch_size is not None:\n",
    "        batch_ix = np.random.choice(len(data), batch_size, replace=replace)\n",
    "        negative_ix = np.random.choice(len(data), batch_size, replace=True)\n",
    "    else:\n",
    "        batch_ix = range(len(data))\n",
    "        negative_ix = np.random.permutation(np.arange(len(data)))\n",
    "\n",
    "    \n",
    "    anchors, positives = zip(*[data[i] for i in batch_ix])\n",
    "    \n",
    "    # sample random rows as negatives.\n",
    "    # Note: you can do better by sampling \"hard\" negatives\n",
    "    negatives = [data[i][1] for i in negative_ix]\n",
    "    \n",
    "    anchors, positives, negatives = map(lambda x: Variable(torch.LongTensor(as_matrix(x, max_len=max_len)),\n",
    "                                                           volatile=volatile), \n",
    "                                        [anchors, positives, negatives])\n",
    "    return anchors, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dummy_anchors, _dummy_positives, _dummy_negatives = generate_batch(train_data, 2)\n",
    "\n",
    "print(\"Q:\")\n",
    "print(_dummy_anchors)\n",
    "print(\"A+:\")\n",
    "print(_dummy_positives)\n",
    "print(\"A-:\")\n",
    "print(_dummy_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(anchors, positives, negatives, delta=1):\n",
    "    \"\"\" \n",
    "    Compute the triplet loss:\n",
    "    \n",
    "    max(0, delta + sim(anchors, negatives) - sim(anchors, positives))\n",
    "    \n",
    "    where sim is a dot-product between vectorized inputs\n",
    "    \n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "    return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_recall(anchors, positives, negatives, delta=1):\n",
    "    \"\"\"\n",
    "    Compute the probability (ratio) at which sim(anchors, negatives) is greater than sim(anchors, positives)\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "    return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_loss(_dummy_anchors, _dummy_positives, _dummy_negatives))\n",
    "print(compute_recall(_dummy_anchors, _dummy_positives, _dummy_negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "max_len = 100\n",
    "batch_size = 32\n",
    "batches_per_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "def iterate_minibatches(data, batch_size=32, max_len=None,\n",
    "                        max_batches=None, shuffle=True, verbose=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    if max_batches is not None:\n",
    "        indices = indices[: batch_size * max_batches]\n",
    "        \n",
    "    irange = tnrange if verbose else range\n",
    "    \n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        yield generate_batch([data[i] for i in indices[start : start + batch_size]], max_len=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a difference, we'll ask __you__ to implement training loop this time.\n",
    "\n",
    "Here's a sketch of one epoch:\n",
    "1. iterate over __`batches_per_epoch`__ batches from __`train_data`__\n",
    "    * Compute loss, backprop, optimize\n",
    "    * Compute and accumulate recall\n",
    "    \n",
    "2. iterate over __`batches_per_epoch`__ batches from __`val_data`__\n",
    "    * Compute and accumulate recall\n",
    "    \n",
    "3. print stuff :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {
    "69ee5b52104d471ca7bfb32ba4309743": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "7b18f460e231498eaafa7653026e98e0": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
